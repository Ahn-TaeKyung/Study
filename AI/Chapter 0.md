back propagation 과 딥러닝으로 작년에 노벨상 받은 교수님 두분 있다고 했지? 인생이 그래 살면서 조금씩 하면돼 오늘 족므하고 내일 조금하고 그러면 돼 호숫가에 조약돌 한웅큼 던져봐 리플현상이 마아아악 일어나다가 나중가서 머지하잖아 그런거야
앞으로 100년이나 남았어 고민하지마 그냥 하면돼 지금은 물리적 개요가 먼저야 수학적인 공식은 나중이야 그냥 맏아들이고 의문을 가져 모르는건 gpt한테 물어봐 그리고 이해해 수학적인 공식은 나중이야 요즘은 트리구조로 공부해야해 탑다운 방식으로 그래서 gpt를 잘써야해
y(n) = x(n) - ax(n-1)
x(n)은 레퍼런스
ex) 사람들은 다 각자의 키가 있어 근데 바닥은 다를 수 있어 n이 현재위치야 n-1은 뭘까 n이 현재위치가 n-1은 내 옆이야 근데 왜 n-1일까 무슨 의미 일까 내 옆 위치에서 내 위치로 이동했으니까 과거란 뜻으로 - 야 그럼 왜 1일까
아날로그 데이터를 디지털 데이터로 바꾸는 개념중에 샘플링이란게 있어 딱딱딱딱 자르는거야 그럼 어느 단위로 잘라 아주 작은 단위로 짤라야해 전화를 하면 말하는걸 바로바로 전환해서 보내야하니까 아주 많은 데이터를 빠르게 처리해야한단말이야
1초에 8000개의 데이터를 처리해 그럼 -1은 뭐야 1/8000 초 전에 위치라는 거야
x(n-2)는? 2/8000 초 전이라는 거야
이걸 왜 공부 할까? 이걸 모르면 나중에 CNN할때 이 개념을 모르면 그냥 산수만 하는거야 의미를 이해하지 못하게되는거야
내가 수업할때도 간혹가다 점프해서 얘기할거야 왜? 미리미리 생각을 하라고

<img src="https://github.com/user-attachments/assets/7dc61d31-d9ee-4c75-9866-704a1dabf61e" width="500"> 




몬테카를로 트리 탐색(MCTS, Monte Carlo Tree Search)은 주로 게임 인공지능에서 사용되는 탐색 알고리즘으로, 게임 트리나 상태 공간에서 최적의 결정을 내리기 위해 시뮬레이션을 반복적으로 실행하는 방법이에요. 주로 보드 게임(예: 체스, 바둑, 틱택토)에서 강력하게 활용됩니다.

MCTS는 네 가지 주요 단계로 구성되어 있어요:

선택 (Selection):

현재 트리에서 최적의 노드를 선택해가며 내려가요. 이때 선택 기준은 탐색을 진행하는 데 있어 더 많은 보상을 받을 가능성이 있는 노드를 선택하려고 해요.

**UCT (Upper Confidence Bounds for Trees)**라는 공식을 사용해 노드를 선택하는데, 이 공식을 통해 이미 방문한 노드의 보상(승리, 패배 등)을 최대화하려고 하고, 덜 탐색된 노드도 고려해요.

확장 (Expansion):

선택한 노드에서 더 이상 자식 노드가 없다면, 새로운 자식 노드를 생성하고 트리를 확장해요.

주로 가능한 행동들이나 게임의 상태에 따른 가능한 선택들이 확장돼요.

시뮬레이션 (Simulation):

새로운 자식 노드에 대해 무작위 시뮬레이션을 진행해요. 이 단계는 게임의 나머지 부분을 예측하는 것이에요.

주로 랜덤한 선택을 통해 게임이 끝날 때까지 시뮬레이션을 진행하고, 그 결과를 통해 승리, 패배, 무승부 등의 결과를 얻어요.

백업 (Backpropagation):

시뮬레이션이 끝난 후, 그 결과를 트리 위로 전달해요. 시뮬레이션의 승패 정보는 부모 노드들로 역전파되며, 각 노드의 승률을 갱신하는 데 사용돼요.

MCTS의 장점
적응성: 초기에는 무작위 탐색을 통해 게임 트리의 구조를 학습하며 점차적으로 더 정교한 탐색을 해요.

시간 효율성: 완전 탐색보다 적은 시간에 좋은 결과를 얻을 수 있어요.

유연성: 다양한 종류의 게임이나 문제에 적용할 수 있어요.

예시
바둑에서는 MCTS가 각 수를 두기 전에 가능한 상태를 시뮬레이션하여 승리할 확률이 높은 수를 선택해요. 이 과정이 반복되며 게임을 진행하게 되죠.
